import { useState, useRef, useCallback, useEffect } from "react";
import { saveFile, exists } from "@/lib/tauri";
import { useFileStore } from "@/stores/useFileStore";
import { useAIStore } from "@/stores/useAIStore";
import { callLLM, type Message } from "@/services/llm";

/**
 * è¯­éŸ³ç¬”è®° Hook
 * æŒç»­å½•éŸ³ï¼Œç»“æŸåä¿å­˜ä¸º markdown æ–‡ä»¶å¹¶è‡ªåŠ¨ç”Ÿæˆ AI æ€»ç»“
 */
export function useVoiceNote() {
  const [isRecording, setIsRecording] = useState(false);
  const [interimText, setInterimText] = useState(""); // å®æ—¶ä¸­é—´ç»“æœ
  const [transcriptChunks, setTranscriptChunks] = useState<string[]>([]); // å·²ç¡®è®¤çš„æ–‡å­—ç‰‡æ®µ
  const [status, setStatus] = useState<"idle" | "recording" | "saving" | "summarizing">("idle");
  
  const recognitionRef = useRef<any>(null);
  const silenceTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  
  const { vaultPath, refreshFileTree, openFile } = useFileStore();
  const { config } = useAIStore();

  // æ¸…é™¤é™éŸ³è®¡æ—¶å™¨
  const clearSilenceTimer = useCallback(() => {
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
  }, []);

  // é‡ç½®é™éŸ³è®¡æ—¶å™¨ï¼ˆ30ç§’æ— å£°éŸ³è‡ªåŠ¨åœæ­¢ï¼Œæ¯”æ™®é€šè¾“å…¥é•¿ï¼‰
  const resetSilenceTimer = useCallback(() => {
    clearSilenceTimer();
    silenceTimerRef.current = setTimeout(() => {
      const recognition = recognitionRef.current;
      if (recognition) {
        recognition.stop();
      }
    }, 30000); // 30ç§’
  }, [clearSilenceTimer]);

  // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
  useEffect(() => {
    const SpeechRecognitionImpl =
      (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    
    if (!SpeechRecognitionImpl) {
      recognitionRef.current = null;
      return;
    }

    const recognition = new SpeechRecognitionImpl();
    recognition.lang = "zh-CN";
    recognition.interimResults = true;
    recognition.continuous = true;

    recognition.onresult = (event: any) => {
      let interim = "";
      let finalText = "";
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalText += transcript;
        } else {
          interim += transcript;
        }
      }
      
      // æ›´æ–°ä¸­é—´ç»“æœ
      setInterimText(interim);
      
      // æœ‰è¯´è¯æ´»åŠ¨æ—¶é‡ç½®è®¡æ—¶å™¨
      resetSilenceTimer();
      
      // æœ€ç»ˆç»“æœè¿½åŠ åˆ°æ–‡å­—ç‰‡æ®µ
      if (finalText) {
        setTranscriptChunks(prev => [...prev, finalText]);
        setInterimText("");
      }
    };

    recognition.onend = () => {
      clearSilenceTimer();
      // å¦‚æœè¿˜åœ¨å½•éŸ³çŠ¶æ€ï¼Œè¯´æ˜æ˜¯æ„å¤–ä¸­æ–­ï¼Œå°è¯•é‡å¯
      if (recognitionRef.current?._shouldContinue) {
        try {
          recognition.start();
        } catch (e) {
          console.error("Failed to restart recognition", e);
          setIsRecording(false);
          setStatus("idle");
        }
      } else {
        setIsRecording(false);
      }
    };

    recognition.onerror = (event: any) => {
      console.error("Speech recognition error:", event.error);
      clearSilenceTimer();
      // å¦‚æœæ˜¯ no-speech é”™è¯¯ï¼Œä¸åœæ­¢å½•éŸ³
      if (event.error === "no-speech") {
        resetSilenceTimer();
        return;
      }
      if (event.error === "not-allowed" || event.error === "service-not-allowed") {
        alert("è¯­éŸ³è¾“å…¥éœ€è¦å¼€å¯éº¦å…‹é£å’Œè¯­éŸ³è¯†åˆ«æƒé™ï¼Œè¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­æˆæƒã€‚");
      } else if (event.error === "audio-capture") {
        alert("æœªæ£€æµ‹åˆ°éº¦å…‹é£è®¾å¤‡ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£è¿æ¥æˆ–ç³»ç»Ÿè®¾ç½®ã€‚");
      } else if (event.error === "network") {
        alert("è¯­éŸ³è¯†åˆ«éœ€è¦è”ç½‘ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚");
      }
      setIsRecording(false);
      setStatus("idle");
    };

    recognitionRef.current = recognition;

    return () => {
      clearSilenceTimer();
      recognition.stop();
    };
  }, [resetSilenceTimer, clearSilenceTimer]);

  // ç”Ÿæˆæ–‡ä»¶åï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰
  const generateFileName = useCallback(() => {
    const now = new Date();
    const year = now.getFullYear();
    const month = String(now.getMonth() + 1).padStart(2, "0");
    const day = String(now.getDate()).padStart(2, "0");
    const hours = String(now.getHours()).padStart(2, "0");
    const minutes = String(now.getMinutes()).padStart(2, "0");
    return `è¯­éŸ³ç¬”è®°_${year}-${month}-${day}_${hours}-${minutes}`;
  }, []);

  // ç”Ÿæˆå”¯ä¸€æ–‡ä»¶è·¯å¾„
  const getUniqueFilePath = useCallback(async (baseName: string) => {
    if (!vaultPath) return null;
    
    const sep = vaultPath.includes("\\") ? "\\" : "/";
    let filePath = `${vaultPath}${sep}${baseName}.md`;
    let counter = 1;
    
    while (await exists(filePath)) {
      filePath = `${vaultPath}${sep}${baseName}_${counter}.md`;
      counter++;
    }
    
    return filePath;
  }, [vaultPath]);

  // è°ƒç”¨ AI ç”Ÿæˆæ€»ç»“
  const generateSummary = useCallback(async (transcript: string): Promise<string> => {
    if (!config.apiKey && config.provider !== "ollama") {
      return "";
    }

    try {
      const messages: Message[] = [
        {
          role: "system",
          content: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¬”è®°åŠ©æ‰‹ã€‚è¯·ä¸ºä»¥ä¸‹è¯­éŸ³è½¬å½•çš„æ–‡å­—ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼Œæå–å…³é”®è¦ç‚¹ã€‚æ€»ç»“åº”è¯¥ç®€æ˜æ‰¼è¦ï¼Œä½¿ç”¨ markdown æ ¼å¼ï¼ŒåŒ…å«è¦ç‚¹åˆ—è¡¨ã€‚"
        },
        {
          role: "user",
          content: `è¯·ä¸ºä»¥ä¸‹è¯­éŸ³ç¬”è®°ç”Ÿæˆæ€»ç»“ï¼š\n\n${transcript}`
        }
      ];
      
      const response = await callLLM(messages, { temperature: 0.3 });
      return response.content || "";
    } catch (error) {
      console.error("Failed to generate summary:", error);
      return "";
    }
  }, [config]);

  // å¼€å§‹å½•éŸ³
  const ensureMicPermission = useCallback(async () => {
    if (!navigator.mediaDevices?.getUserMedia) return true;
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach((track) => track.stop());
      return true;
    } catch (err) {
      console.error("Microphone permission denied", err);
      alert("æ— æ³•è·å–éº¦å…‹é£æƒé™ï¼Œè¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­å…è®¸éº¦å…‹é£è®¿é—®ã€‚");
      return false;
    }
  }, []);

  const startRecording = useCallback(async () => {
    const recognition = recognitionRef.current;
    if (!recognition) {
      alert("å½“å‰ç¯å¢ƒä¸æ”¯æŒè¯­éŸ³è¾“å…¥");
      return;
    }

    if (!vaultPath) {
      alert("è¯·å…ˆæ‰“å¼€ä¸€ä¸ªå·¥ä½œåŒº");
      return;
    }

    // é‡ç½®çŠ¶æ€
    setTranscriptChunks([]);
    setInterimText("");
    setStatus("recording");
    
    try {
      const ok = await ensureMicPermission();
      if (!ok) {
        setIsRecording(false);
        setStatus("idle");
        return;
      }
      recognition._shouldContinue = true;
      recognition.start();
      setIsRecording(true);
      resetSilenceTimer();
    } catch (e) {
      console.error("Failed to start speech recognition", e);
      setIsRecording(false);
      setStatus("idle");
    }
  }, [vaultPath, resetSilenceTimer, ensureMicPermission]);

  // åœæ­¢å½•éŸ³å¹¶ä¿å­˜
  const stopRecording = useCallback(async () => {
    const recognition = recognitionRef.current;
    if (recognition) {
      recognition._shouldContinue = false;
      recognition.stop();
    }
    
    clearSilenceTimer();
    setIsRecording(false);
    
    // åˆå¹¶æ‰€æœ‰æ–‡å­—ç‰‡æ®µ
    const fullTranscript = transcriptChunks.join("");
    
    if (!fullTranscript.trim()) {
      setStatus("idle");
      setTranscriptChunks([]);
      return null;
    }

    setStatus("saving");

    try {
      // ç”Ÿæˆæ–‡ä»¶
      const fileName = generateFileName();
      const filePath = await getUniqueFilePath(fileName);
      
      if (!filePath) {
        throw new Error("æ— æ³•ç”Ÿæˆæ–‡ä»¶è·¯å¾„");
      }

      // æ„å»ºåˆå§‹å†…å®¹
      const now = new Date();
      const dateStr = now.toLocaleString("zh-CN");
      let content = `# ${fileName}\n\n`;
      content += `> ğŸ“… åˆ›å»ºæ—¶é—´ï¼š${dateStr}\n\n`;
      content += `## åŸå§‹æ–‡ç¨¿\n\n${fullTranscript}\n`;

      // å…ˆä¿å­˜åŸå§‹æ–‡ç¨¿
      await saveFile(filePath, content);
      await refreshFileTree();

      // ç”Ÿæˆ AI æ€»ç»“
      setStatus("summarizing");
      const summary = await generateSummary(fullTranscript);
      
      if (summary) {
        // è¿½åŠ æ€»ç»“åˆ°æ–‡ä»¶
        content += `\n---\n\n## AI æ€»ç»“\n\n${summary}\n`;
        await saveFile(filePath, content);
      }

      // åˆ·æ–°æ–‡ä»¶æ ‘å¹¶æ‰“å¼€æ–‡ä»¶
      await refreshFileTree();
      // ç¨ç­‰ä¸€ä¸‹ç¡®ä¿æ–‡ä»¶æ ‘æ›´æ–°å®Œæˆ
      await new Promise(resolve => setTimeout(resolve, 100));
      openFile(filePath);
      
      setStatus("idle");
      setTranscriptChunks([]);
      
      return filePath;
    } catch (error) {
      console.error("Failed to save voice note:", error);
      alert("ä¿å­˜è¯­éŸ³ç¬”è®°å¤±è´¥");
      setStatus("idle");
      setTranscriptChunks([]);
      return null;
    }
  }, [transcriptChunks, generateFileName, getUniqueFilePath, generateSummary, refreshFileTree, openFile, clearSilenceTimer]);

  // å–æ¶ˆå½•éŸ³
  const cancelRecording = useCallback(() => {
    const recognition = recognitionRef.current;
    if (recognition) {
      recognition._shouldContinue = false;
      recognition.stop();
    }
    
    clearSilenceTimer();
    setIsRecording(false);
    setStatus("idle");
    setTranscriptChunks([]);
    setInterimText("");
  }, [clearSilenceTimer]);

  // å½“å‰å·²å½•å…¥çš„æ–‡å­—ï¼ˆå®æ—¶æ˜¾ç¤ºç”¨ï¼‰
  const currentTranscript = transcriptChunks.join("") + interimText;

  return {
    isRecording,
    status,
    interimText,
    currentTranscript,
    startRecording,
    stopRecording,
    cancelRecording,
  };
}
