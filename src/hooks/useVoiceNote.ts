import { useState, useRef, useCallback, useEffect } from "react";
import { saveFile, exists } from "@/lib/tauri";
import { useFileStore } from "@/stores/useFileStore";
import { useAIStore } from "@/stores/useAIStore";
import { useLocaleStore } from "@/stores/useLocaleStore";
import { callLLM, type Message } from "@/services/llm";
import { reportOperationError } from "@/lib/reportError";

const isMacSpeechBlockedInDev = () => {
  if (!import.meta.env.DEV) return false;
  if (typeof navigator === "undefined") return false;
  const isMac = /Mac/i.test(navigator.userAgent);
  const isDevServer = window.location.protocol.startsWith("http");
  return isMac && isDevServer;
};

/**
 * è¯­éŸ³ç¬”è®° Hook
 * æŒç»­å½•éŸ³ï¼Œç»“æŸåä¿å­˜ä¸º markdown æ–‡ä»¶å¹¶è‡ªåŠ¨ç”Ÿæˆ AI æ€»ç»“
 */
export function useVoiceNote() {
  const [isRecording, setIsRecording] = useState(false);
  const [interimText, setInterimText] = useState(""); // å®æ—¶ä¸­é—´ç»“æœ
  const [transcriptChunks, setTranscriptChunks] = useState<string[]>([]); // å·²ç¡®è®¤çš„æ–‡å­—ç‰‡æ®µ
  const [status, setStatus] = useState<"idle" | "recording" | "saving" | "summarizing">("idle");
  
  const recognitionRef = useRef<any>(null);
  const silenceTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  
  const { vaultPath, refreshFileTree, openFile } = useFileStore();
  const { config } = useAIStore();
  const { t, locale } = useLocaleStore();

  // æ¸…é™¤é™éŸ³è®¡æ—¶å™¨
  const clearSilenceTimer = useCallback(() => {
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
  }, []);

  // é‡ç½®é™éŸ³è®¡æ—¶å™¨ï¼ˆ30ç§’æ— å£°éŸ³è‡ªåŠ¨åœæ­¢ï¼Œæ¯”æ™®é€šè¾“å…¥é•¿ï¼‰
  const resetSilenceTimer = useCallback(() => {
    clearSilenceTimer();
    silenceTimerRef.current = setTimeout(() => {
      const recognition = recognitionRef.current;
      if (recognition) {
        recognition.stop();
      }
    }, 30000); // 30ç§’
  }, [clearSilenceTimer]);

  // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
  useEffect(() => {
    const SpeechRecognitionImpl =
      (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    
    if (!SpeechRecognitionImpl) {
      recognitionRef.current = null;
      return;
    }

    const recognition = new SpeechRecognitionImpl();
    recognition.lang = "zh-CN";
    recognition.interimResults = true;
    recognition.continuous = true;

    recognition.onresult = (event: any) => {
      let interim = "";
      let finalText = "";
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalText += transcript;
        } else {
          interim += transcript;
        }
      }
      
      // æ›´æ–°ä¸­é—´ç»“æœ
      setInterimText(interim);
      
      // æœ‰è¯´è¯æ´»åŠ¨æ—¶é‡ç½®è®¡æ—¶å™¨
      resetSilenceTimer();
      
      // æœ€ç»ˆç»“æœè¿½åŠ åˆ°æ–‡å­—ç‰‡æ®µ
      if (finalText) {
        setTranscriptChunks(prev => [...prev, finalText]);
        setInterimText("");
      }
    };

    recognition.onend = () => {
      clearSilenceTimer();
      // å¦‚æœè¿˜åœ¨å½•éŸ³çŠ¶æ€ï¼Œè¯´æ˜æ˜¯æ„å¤–ä¸­æ–­ï¼Œå°è¯•é‡å¯
      if (recognitionRef.current?._shouldContinue) {
        try {
          recognition.start();
        } catch (e) {
          reportOperationError({
            source: "useVoiceNote.recognition.onend",
            action: "Restart speech recognition",
            error: e,
            level: "warning",
          });
          setIsRecording(false);
          setStatus("idle");
        }
      } else {
        setIsRecording(false);
      }
    };

    recognition.onerror = (event: any) => {
      console.error("Speech recognition error:", event.error);
      clearSilenceTimer();
      // å¦‚æœæ˜¯ no-speech é”™è¯¯ï¼Œä¸åœæ­¢å½•éŸ³
      if (event.error === "no-speech") {
        resetSilenceTimer();
        return;
      }
      if (event.error === "not-allowed" || event.error === "service-not-allowed") {
        alert(t.speech.permissionRequired);
      } else if (event.error === "audio-capture") {
        alert(t.speech.noMic);
      } else if (event.error === "network") {
        alert(t.speech.networkRequired);
      }
      setIsRecording(false);
      setStatus("idle");
    };

    recognitionRef.current = recognition;

    return () => {
      clearSilenceTimer();
      recognition.stop();
    };
  }, [resetSilenceTimer, clearSilenceTimer]);

  // ç”Ÿæˆæ–‡ä»¶åï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰
  const generateFileName = useCallback(() => {
    const now = new Date();
    const year = now.getFullYear();
    const month = String(now.getMonth() + 1).padStart(2, "0");
    const day = String(now.getDate()).padStart(2, "0");
    const hours = String(now.getHours()).padStart(2, "0");
    const minutes = String(now.getMinutes()).padStart(2, "0");
    return `${t.file.voiceNotePrefix}_${year}-${month}-${day}_${hours}-${minutes}`;
  }, [t.file.voiceNotePrefix]);

  // ç”Ÿæˆå”¯ä¸€æ–‡ä»¶è·¯å¾„
  const getUniqueFilePath = useCallback(async (baseName: string) => {
    if (!vaultPath) return null;
    
    const sep = vaultPath.includes("\\") ? "\\" : "/";
    let filePath = `${vaultPath}${sep}${baseName}.md`;
    let counter = 1;
    
    while (await exists(filePath)) {
      filePath = `${vaultPath}${sep}${baseName}_${counter}.md`;
      counter++;
    }
    
    return filePath;
  }, [vaultPath]);

  // è°ƒç”¨ AI ç”Ÿæˆæ€»ç»“
  const generateSummary = useCallback(async (transcript: string): Promise<string> => {
    if (!config.apiKey && config.provider !== "ollama") {
      return "";
    }

    try {
      const messages: Message[] = [
        {
          role: "system",
          content: t.speech.voiceNoteSummarySystem,
        },
        {
          role: "user",
          content: t.speech.voiceNoteSummaryUser.replace('{text}', transcript),
        }
      ];
      
      const response = await callLLM(messages, { temperature: 0.3 });
      return response.content || "";
    } catch (error) {
      reportOperationError({
        source: "useVoiceNote.generateSummary",
        action: "Generate voice note summary",
        error,
        level: "warning",
      });
      return "";
    }
  }, [config, t]);

  // å¼€å§‹å½•éŸ³
  const ensureMicPermission = useCallback(async () => {
    if (!navigator.mediaDevices?.getUserMedia) return true;
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach((track) => track.stop());
      return true;
    } catch (err) {
      reportOperationError({
        source: "useVoiceNote.ensureMicPermission",
        action: "Request microphone permission",
        error: err,
        level: "warning",
      });
      alert(t.speech.permissionDenied);
      return false;
    }
  }, [t.speech.permissionDenied]);

  const startRecording = useCallback(async () => {
    const recognition = recognitionRef.current;
    if (isMacSpeechBlockedInDev()) {
      alert(t.speech.macDevWarning);
      return;
    }
    if (!recognition) {
      alert(t.speech.unsupported);
      return;
    }

    if (!vaultPath) {
      alert(t.common.openWorkspaceFirst);
      return;
    }

    // é‡ç½®çŠ¶æ€
    setTranscriptChunks([]);
    setInterimText("");
    setStatus("recording");
    
    try {
      const ok = await ensureMicPermission();
      if (!ok) {
        setIsRecording(false);
        setStatus("idle");
        return;
      }
      recognition._shouldContinue = true;
      recognition.start();
      setIsRecording(true);
      resetSilenceTimer();
    } catch (e) {
      reportOperationError({
        source: "useVoiceNote.startRecording",
        action: "Start speech recognition",
        error: e,
      });
      setIsRecording(false);
      setStatus("idle");
    }
  }, [vaultPath, resetSilenceTimer, ensureMicPermission, t]);

  // åœæ­¢å½•éŸ³å¹¶ä¿å­˜
  const stopRecording = useCallback(async () => {
    const recognition = recognitionRef.current;
    if (recognition) {
      recognition._shouldContinue = false;
      recognition.stop();
    }
    
    clearSilenceTimer();
    setIsRecording(false);
    
    // åˆå¹¶æ‰€æœ‰æ–‡å­—ç‰‡æ®µ
    const fullTranscript = transcriptChunks.join("");
    
    if (!fullTranscript.trim()) {
      setStatus("idle");
      setTranscriptChunks([]);
      return null;
    }

    setStatus("saving");

    try {
      // ç”Ÿæˆæ–‡ä»¶
      const fileName = generateFileName();
      const filePath = await getUniqueFilePath(fileName);
      
      if (!filePath) {
        throw new Error(t.file.voiceNotePathFailed);
      }

      // æ„å»ºåˆå§‹å†…å®¹
      const now = new Date();
      const dateStr = now.toLocaleString(locale);
      let content = `# ${fileName}\n\n`;
      content += `> ğŸ“… ${t.file.voiceNoteCreatedAtLabel}ï¼š${dateStr}\n\n`;
      content += `## ${t.file.voiceNoteTranscriptTitle}\n\n${fullTranscript}\n`;

      // å…ˆä¿å­˜åŸå§‹æ–‡ç¨¿
      await saveFile(filePath, content);
      await refreshFileTree();

      // ç”Ÿæˆ AI æ€»ç»“
      setStatus("summarizing");
      const summary = await generateSummary(fullTranscript);
      
      if (summary) {
        // è¿½åŠ æ€»ç»“åˆ°æ–‡ä»¶
        content += `\n---\n\n## ${t.file.voiceNoteSummaryTitle}\n\n${summary}\n`;
        await saveFile(filePath, content);
      }

      // åˆ·æ–°æ–‡ä»¶æ ‘å¹¶æ‰“å¼€æ–‡ä»¶
      await refreshFileTree();
      // ç¨ç­‰ä¸€ä¸‹ç¡®ä¿æ–‡ä»¶æ ‘æ›´æ–°å®Œæˆ
      await new Promise(resolve => setTimeout(resolve, 100));
      openFile(filePath);
      
      setStatus("idle");
      setTranscriptChunks([]);
      
      return filePath;
    } catch (error) {
      reportOperationError({
        source: "useVoiceNote.stopRecording",
        action: "Save voice note",
        error,
        userMessage: t.file.voiceNoteSaveFailed,
      });
      setStatus("idle");
      setTranscriptChunks([]);
      return null;
    }
  }, [transcriptChunks, generateFileName, getUniqueFilePath, generateSummary, refreshFileTree, openFile, clearSilenceTimer, t, locale]);

  // å–æ¶ˆå½•éŸ³
  const cancelRecording = useCallback(() => {
    const recognition = recognitionRef.current;
    if (recognition) {
      recognition._shouldContinue = false;
      recognition.stop();
    }
    
    clearSilenceTimer();
    setIsRecording(false);
    setStatus("idle");
    setTranscriptChunks([]);
    setInterimText("");
  }, [clearSilenceTimer]);

  // å½“å‰å·²å½•å…¥çš„æ–‡å­—ï¼ˆå®æ—¶æ˜¾ç¤ºç”¨ï¼‰
  const currentTranscript = transcriptChunks.join("") + interimText;

  return {
    isRecording,
    status,
    interimText,
    currentTranscript,
    startRecording,
    stopRecording,
    cancelRecording,
  };
}
